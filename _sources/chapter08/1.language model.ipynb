{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 语言模型\n",
    "\n",
    "```{note}\n",
    "假设长度为 $T$ 的文本序列中的词元（tokens）依次为 $x_{1},x_{2},...,x_{T}$。<br/>\n",
    "语言模型（language model）的目标是估计序列的联合概率：\n",
    "\n",
    "$$P(x_{1},x_{2},...,x_{T})$$\n",
    "\n",
    "语言模型非常有用。列如，只要一次抽取一个词元：\n",
    "\n",
    "$$x_{t} \\sim P(x_{t}|x_{1},...,x_{t-1})$$\n",
    "\n",
    "一个理想的语言模型就能够基于模型本身生成自然的文本。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram模型\n",
    "\n",
    "如何计算联合概率呢？我们首先将它拆解为条件概率的乘积：\n",
    "\n",
    "$$P(x_{1},...,x_{T}) = \\prod_{t=1}^{T}P(x_{t}|x_{1},...,x_{t-1})$$\n",
    "\n",
    "如果我们有一个大型的语料库，那么条件概率可以由频次之比来估计，比如说:\n",
    "\n",
    "$$\\hat{P}(\\mbox{learning}|\\mbox{deep}) = \\frac{n(\\mbox{deep}, \\mbox{learning})}{n(\\mbox{deep})}$$\n",
    "\n",
    "长段的连续词元会有出现频次过少的问题，因此N-gram模型假设我们的文本具有 $N$-阶 Markov 性质:\n",
    "\n",
    "$$P(x_{t}|x_{1},...,x_{t-1}) = P(x_{t}|x_{t-N+1},...,x_{t-1})$$\n",
    "\n",
    "即最多只依赖 $N-1$ 个词元。bigram 和 trigram 模型:\n",
    "\n",
    "$$P(x_{1}, x_{2}, x_{3}, x_{4}) = P(x_{1})P(x_{2}|x_{1})P(x_{3}|x_{2})P(x_{4}|x_{3})$$\n",
    "\n",
    "$$P(x_{1}, x_{2}, x_{3}, x_{4}) = P(x_{1})P(x_{2}|x_{1})P(x_{3}|x_{1},x_{2})P(x_{4}|x_{2},x_{3})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laplace平滑\n",
    "\n",
    "即使我们假设文本具有Markov性质，不常见的连续词元也会有出现频次过少的问题（甚至为0）。\n",
    "\n",
    "一个常见的应对策略是使用Laplace平滑，具体方法是分子分母中各加上一个小常量：\n",
    "\n",
    "$$\\hat{P}(x) = \\frac{n(x) + \\epsilon_{1}/m}{n + \\epsilon_{1}}$$\n",
    "\n",
    "$$\\hat{P}(x'|x)=\\frac{n(x,x') + \\epsilon_{2}\\hat{P}(x')}{n(x) + \\epsilon_{2}}$$\n",
    "\n",
    "$$\\hat{P}(x''|x,x')=\\frac{n(x, x', x'') + \\epsilon_{3}\\hat{P}(x'')}{n(x, x') + \\epsilon_{3}}$$\n",
    "\n",
    "这里$n$是语料库的单词总数，$m$是不同的单词数，$\\epsilon_{1},\\epsilon_{2},\\epsilon_{3}$是超参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 困惑度（Perplexity）\n",
    "\n",
    "如何衡量一个语言模型的好坏呢？\n",
    "\n",
    "直觉上，一个好的语言模型应该能够让我们更准确的预测下一个token，对于一个共n个词元的真实序列，这可以通过平均交叉熵损失函数来衡量:\n",
    "\n",
    "$$\\frac{1}{n}\\sum_{t=1}^{n}-\\mbox{log}\\ P(x_{t}|x_{t-1},...,x_{1})$$\n",
    "\n",
    "困惑度（Perplexity）就是上述量的指数:\n",
    "\n",
    "$$\\mbox{exp}\\left (\\frac{1}{n}\\sum_{t=1}^{n}-\\mbox{log}\\ P(x_{t}|x_{t-1},...,x_{1})\\right )$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
