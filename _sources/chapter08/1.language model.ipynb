{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model\n",
    "\n",
    "Assume that the tokens in a text sequence of length  $T$  are in turn  $x_{1},x_{2},...,x_{T}$.\n",
    "\n",
    "The goal of language model is to estimate the joint-probability of the sequence:\n",
    "\n",
    "$$P(x_{1},x_{2},...,x_{T})$$\n",
    "\n",
    "An ideal language model would be able to generate natural text just on its own, simply by drawing one token at a time:\n",
    "\n",
    "$$x_{t} \\sim P(x_{t}|x_{1},...,x_{t-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram\n",
    "\n",
    "How to compute the joint-probabilty? start by applying basic probability rules:\n",
    "\n",
    "$$P(x_{1},...,x_{T}) = \\prod_{t=1}^{T}P(x_{t}|x_{1},...,x_{t-1})$$\n",
    "\n",
    "While probability could be estimate by frequencies:\n",
    "\n",
    "$$\\hat{P}(\\mbox{learning}|\\mbox{deep}) = \\frac{n(\\mbox{deep}, \\mbox{learning})}{n(\\mbox{deep})}$$\n",
    "\n",
    "Further more, Laplace smoothing:\n",
    "\n",
    "$$\\hat{P}(x) = \\frac{n(x) + \\epsilon_{1}/m}{n + \\epsilon_{1}}$$\n",
    "\n",
    "$$\\hat{P}(x'|x)=\\frac{n(x,x') + \\epsilon_{2}\\hat{P}(x')}{n(x) + \\epsilon_{2}}$$\n",
    "\n",
    "$$\\hat{P}(x''|x,x')=\\frac{n(x, x', x'') + \\epsilon_{3}\\hat{P}(x'')}{n(x, x') + \\epsilon_{3}}$$\n",
    "\n",
    "On the other hand, Assume $N$-order Markov property:\n",
    "\n",
    "$$P(x_{t}|x_{1},...,x_{t-1}) = P(x_{t}|x_{t-N+1},...,x_{t-1})$$\n",
    "\n",
    "We arrive at the $N$-gram language model, e.g. the bigram and trigram model:\n",
    "\n",
    "$$P(x_{1}, x_{2}, x_{3}, x_{4}) = P(x_{1})P(x_{2}|x_{1})P(x_{3}|x_{2})P(x_{4}|x_{3})$$\n",
    "\n",
    "$$P(x_{1}, x_{2}, x_{3}, x_{4}) = P(x_{1})P(x_{2}|x_{1})P(x_{3}|x_{1},x_{2})P(x_{4}|x_{2},x_{3})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity\n",
    "\n",
    "How to measure the language model quality?\n",
    "\n",
    "A better language model should allow us to predict the next token more accurately.\n",
    "\n",
    "So we can measure it by the cross-entropy loss averaged over all the  $n$  tokens of a sequence:\n",
    "\n",
    "$$\\frac{1}{n}\\sum_{t=1}^{n}-\\mbox{log}\\ P(x_{t}|x_{t-1},...,x_{1})$$\n",
    "\n",
    "where  $x_{t}$  is the actual token, this makes the performance on documents of different lengths comparable.\n",
    "\n",
    "For historical reasons, we use exponentials called Perplexity:\n",
    "\n",
    "$$\\mbox{exp}\\left (\\frac{1}{n}\\sum_{t=1}^{n}-\\mbox{log}\\ P(x_{t}|x_{t-1},...,x_{1})\\right )$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
