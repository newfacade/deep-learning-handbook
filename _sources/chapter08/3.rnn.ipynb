{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 循环神经网络\n",
    "\n",
    "```{note}\n",
    "$n$-gram模型的参数量会随着$n$呈指数增长，因此与其模型化 $P(x_{t}|x_{t-n+1},...,x_{t-1})$，不如使用隐变量模型：\n",
    "\n",
    "$$P(x_{t}|h_{t-1}) \\approx P(x_{t}|x_{1},...,x_{t-1})$$\n",
    "\n",
    "其中 $h_{t-1}$ 存储着到时间步 $t-1$ 的序列信息，这就是循环神经网络（recurrent neural networks，RNN）的思想。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型\n",
    "\n",
    "回想全连接层:\n",
    "\n",
    "$$\\mathbf{H} = \\phi(\\mathbf{X}\\mathbf{W}_{xh} + \\mathbf{b}_{h})$$\n",
    "\n",
    "其中 $\\mathbf{X}\\in\\mathbb{R}^{n\\times{d}}$,$\\mathbf{W}_{xh}\\in\\mathbb{R}^{d\\times{h}}$,$\\mathbf{b}_{h}\\in\\mathbb{R}^{1\\times{h}}$,$\\mathbf{H}\\in\\mathbb{R}^{n\\times{h}}$.\n",
    "\n",
    "引入了时间步的概念之后，事情会变得有些不一样。假设 $t$ 时刻的输入为 $\\mathbf{X}_{t}\\in\\mathbb{R}^{n\\times{d}}$，隐藏状态为  $\\mathbf{H}_{t}\\in\\mathbb{R}^{n\\times{h}}$，那么RNN模型是如此更新隐藏状态的:\n",
    "\n",
    "$$\\mathbf{H}_{t} = \\phi(\\mathbf{X}_{t}\\mathbf{W}_{xh} + \\mathbf{H}_{t-1}\\mathbf{W}_{hh} + \\mathbf{b}_{h})$$\n",
    "\n",
    "其中 $\\mathbf{W}_{hh}\\in\\mathbb{R}^{h\\times{h}}$， 激活函数 $\\phi$ 默认为 $\\mbox{tanh}$。\n",
    "\n",
    "![jupyter](../images/8/rnn.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import d2l\n",
    "\n",
    "\n",
    "#@save\n",
    "class RNNModel(nn.Module):\n",
    "    \"\"\"RNN模型\"\"\"\n",
    "    def __init__(self, rnn_layer, vocab_size, **kwargs):\n",
    "        super(RNNModel, self).__init__(**kwargs)\n",
    "        # 比如说nn.RNN()\n",
    "        self.rnn = rnn_layer\n",
    "        # 词汇量的大小\n",
    "        self.vocab_size = vocab_size\n",
    "        # 输入vocab_size -> 隐藏状态num_hiddens -> 输出vocab_size\n",
    "        self.num_hiddens = self.rnn.hidden_size\n",
    "        # 是否双向\n",
    "        if not self.rnn.bidirectional:\n",
    "            self.num_directions = 1\n",
    "            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)\n",
    "        else:\n",
    "            self.num_directions = 2\n",
    "            self.linear = nn.Linear(self.num_hiddens * 2, self.vocab_size)\n",
    "\n",
    "    def forward(self, inputs, state):\n",
    "        # shape of inputs: (`batch_size`, `num_steps`)\n",
    "        # shape of X: (`num_steps`, `batch_size`, `vocab_size`)\n",
    "        # 将输入的int转为one_hot表示\n",
    "        X = F.one_hot(inputs.T.long(), self.vocab_size).type(torch.float32)\n",
    "        # shape of Y: (`num_steps`, `batch_size`, `num_directions` * `num_hiddens`)\n",
    "        # shape of state: (`num_layers` * `num_directions`, `batch_size`, `num_hiddens`)\n",
    "        # state是最终的隐藏状态\n",
    "        Y, state = self.rnn(X, state)\n",
    "        # shape of output: (`num_steps` * `batch_size`, `vocab_size`)\n",
    "        output = self.linear(Y.reshape((-1, Y.shape[-1])))\n",
    "        return output, state\n",
    "\n",
    "    def begin_state(self, batch_size=1, device=d2l.try_gpu()):\n",
    "        if not isinstance(self.rnn, nn.LSTM):\n",
    "            # `nn.RNN` and `nn.GRU` takes a tensor as hidden state\n",
    "            return torch.zeros((self.num_directions * self.rnn.num_layers,\n",
    "                                batch_size, self.num_hiddens), device=device)\n",
    "        else:\n",
    "            # `nn.LSTM` takes a tuple of hidden states\n",
    "            return (torch.zeros((self.num_directions * self.rnn.num_layers,\n",
    "                                 batch_size, self.num_hiddens), device=device),\n",
    "                    torch.zeros((self.num_directions * self.rnn.num_layers,\n",
    "                                 batch_size, self.num_hiddens), device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNModel(\n",
       "  (rnn): RNN(100, 64)\n",
       "  (linear): Linear(in_features=64, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建一个普通的RNN模型，vocab_size为100, num_hiddens为64\n",
    "rnn = RNNModel(nn.RNN(input_size=100, hidden_size=64), vocab_size=100)\n",
    "rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
