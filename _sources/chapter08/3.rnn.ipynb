{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "\n",
    "Rather than modeling  $P(x_{t}|x_{t-n+1},...,x_{t-1})$  it is preferable to use a latent variable model:\n",
    "\n",
    "$$P(x_{t}|x_{1},...,x_{t-1}) \\approx P(x_{t}|h_{t-1})$$\n",
    "\n",
    "where  $h_{t-1}$  is a hidden state that stores the sequence information up to time step  $t-1$ . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import d2l\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Recall fully-connected layer:\n",
    "\n",
    "$$\\mathbf{H} = \\phi(\\mathbf{X}\\mathbf{W}_{xh} + \\mathbf{b}_{h})$$\n",
    "\n",
    "where $\\mathbf{X}\\in\\mathbb{R}^{n\\times{d}}$,$\\mathbf{W}_{xh}\\in\\mathbb{R}^{d\\times{h}}$,$\\mathbf{b}_{h}\\in\\mathbb{R}^{1\\times{h}}$  and  $\\mathbf{H}\\in\\mathbb{R}^{n\\times{h}}$.\n",
    "\n",
    "Matters are entirely different when we have hidden states, Assume that we have a minibatch of inputs  $\\mathbf{X}_{t}\\in\\mathbb{R}^{n\\times{d}}$  at time step  $t$.\n",
    "\n",
    "Denote by  $\\mathbf{H}_{t}\\in\\mathbb{R}^{n\\times{h}}$  the hidden states of time step  $t$, the RNN model update hidden states by:\n",
    "\n",
    "$$\\mathbf{H}_{t} = \\phi(\\mathbf{X}_{t}\\mathbf{W}_{xh} + \\mathbf{H}_{t-1}\\mathbf{W}_{hh} + \\mathbf{b}_{h})$$\n",
    "\n",
    "where $\\mathbf{W}_{hh}\\in\\mathbb{R}^{h\\times{h}}$ and $\\phi = \\mbox{tanh}$ as default.\n",
    "\n",
    "![jupyter](../images/8/rnn.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "class RNNModel(nn.Module):\n",
    "    \"\"\"The RNN model.\"\"\"\n",
    "    def __init__(self, rnn_layer, vocab_size, **kwargs):\n",
    "        super(RNNModel, self).__init__(**kwargs)\n",
    "        self.rnn = rnn_layer\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_hiddens = self.rnn.hidden_size  # vocab_size -> num_hiddens -> vocab_size\n",
    "        if not self.rnn.bidirectional:\n",
    "            self.num_directions = 1\n",
    "            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)\n",
    "        else:\n",
    "            self.num_directions = 2\n",
    "            self.linear = nn.Linear(self.num_hiddens * 2, self.vocab_size)\n",
    "\n",
    "    def forward(self, inputs, state):\n",
    "        # Shape of inputs: (`batch_size`, `num_steps`)\n",
    "        # Shape of X: (`num_steps`, `batch_size`, `vocab_size`)\n",
    "        X = F.one_hot(inputs.T.long(), self.vocab_size).type(torch.float32)\n",
    "        Y, state = self.rnn(X, state)\n",
    "        # The fully connected layer will first change the shape of `Y` to\n",
    "        # (`num_steps` * `batch_size`, `num_hiddens`). Its output shape is\n",
    "        # (`num_steps` * `batch_size`, `vocab_size`).\n",
    "        output = self.linear(Y.reshape((-1, Y.shape[-1])))\n",
    "        return output, state\n",
    "\n",
    "    def begin_state(self, batch_size=1, device=d2l.try_gpu()):\n",
    "        if not isinstance(self.rnn, nn.LSTM):\n",
    "            # `nn.GRU` takes a tensor as hidden state\n",
    "            return torch.zeros((self.num_directions * self.rnn.num_layers,\n",
    "                                batch_size, self.num_hiddens), device=device)\n",
    "        else:\n",
    "            # `nn.LSTM` takes a tuple of hidden states\n",
    "            return (torch.zeros((self.num_directions * self.rnn.num_layers,\n",
    "                                 batch_size, self.num_hiddens), device=device),\n",
    "                    torch.zeros((self.num_directions * self.rnn.num_layers,\n",
    "                                 batch_size, self.num_hiddens), device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNModel(\n",
       "  (rnn): RNN(100, 64)\n",
       "  (linear): Linear(in_features=64, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"vocab_size: 100, num_hiddens: 64\"\"\"\n",
    "rnn = RNNModel(nn.RNN(input_size=100, hidden_size=64), vocab_size=100)\n",
    "rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
