{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 梯度下降\n",
    "\n",
    "```{note}\n",
    "梯度下降法是最基本的优化算法，但一次更新慢<br/>\n",
    "随机梯度下降一次只使用一个样本的梯度进行更新，但随机性大且数据效率低<br/>\n",
    "小批量随机梯度下降相对最好\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度下降\n",
    "\n",
    "目标函数 $f: \\mathbb{R}^{d} \\to \\mathbb{R}$ 的泰勒展开式为:\n",
    "\n",
    "$$f(\\mathbf{x} + \\epsilon) = f(\\mathbf{x}) + \\epsilon^{T}\\nabla{f(\\mathbf{x})} + \\mathcal{O}(\\left \\| \\epsilon \\right \\|^2)$$\n",
    "\n",
    "由此可以看出，$f$ 下降最快的方向是梯度的反方向  $-\\nabla{f(\\mathbf{x})}$。由此可推导出梯度下降法:\n",
    "\n",
    "$$\\mathbf{x}_{t} = \\mathbf{x}_{t-1} - \\eta\\nabla{f(\\mathbf{x}_{t-1})}$$\n",
    "\n",
    "其中 $\\eta > 0$ 是学习率.\n",
    "\n",
    "![jupyter](../images/c/gd1.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机梯度下降\n",
    "\n",
    "在深度学习中，目标函数是所有训练样本的损失函数的均值:\n",
    "\n",
    "$$f(\\mathbf{x}) = \\frac{1}{n}\\sum_{i=1}^{n}f_{i}(\\mathbf{x})$$\n",
    "\n",
    "梯度:\n",
    "\n",
    "$$\\nabla{f(\\mathbf{x})} = \\frac{1}{n}\\sum_{i=1}^{n}\\nabla{f_{i}(\\mathbf{x})}$$\n",
    "\n",
    "在梯度下降中，每一次更新的时间复杂度为  $\\mathcal{O}(n)$, 当训练集很大时更新会很慢。\n",
    "\n",
    "随机梯度下降（Stochastic gradient descent，SGD） 每次更新会随机选取一个样本，仅使用此样本的梯度进行更新:\n",
    "\n",
    "$$\\mathbf{x}_{t} = \\mathbf{x}_{t-1} - \\eta\\nabla{f_{i}(\\mathbf{x}_{t-1})}$$\n",
    "\n",
    "时间复杂度为 $\\mathcal{O}(1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小批量随机梯度下降\n",
    "\n",
    "梯度下降更新太慢。\n",
    "\n",
    "SGD更新次数太多，随机性大，不能充分使用矢量化加速运算（数据效率低）。\n",
    "\n",
    "小批量随机梯度下降介乎两者之间，每次会使用一小批量（比如说64个、128个等）的样本来进行更新。它可以充分使用矢量化加速运算，随机性也可以接受。\n",
    "\n",
    "$$\\mathbf{x}_{t} = \\mathbf{x}_{t-1} - \\frac{\\eta}{|\\mathcal{B}_{t}|}\\sum_{i\\in\\mathcal{B}_{t}}\\nabla{f_{i}(\\mathbf{x}_{t-1})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "net = nn.Sequential(nn.Linear(3, 1))\n",
    "# 实际上就是小批量随机梯度下降，batch_size在data_iter中定义\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
