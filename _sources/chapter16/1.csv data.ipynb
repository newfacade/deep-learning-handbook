{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV数据\n",
    "\n",
    "```{note}\n",
    "之前我们使用了Fashion-MNIST数据集和加州房价数据集，它们都是在内存中的ndarray。<br/>\n",
    "本节通过一个示例来展示如何加载与预处理 CSV 格式的数据。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "# 文件下载地址\n",
    "TRAIN_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\"\n",
    "TEST_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/eval.csv\"\n",
    "\n",
    "# 下载csv文件\n",
    "train_file_path = keras.utils.get_file(\"train.csv\", TRAIN_DATA_URL)\n",
    "test_file_path = keras.utils.get_file(\"eval.csv\", TEST_DATA_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "survived,sex,age,n_siblings_spouses,parch,fare,class,deck,embark_town,alone\r\n",
      "0,male,22.0,1,0,7.25,Third,unknown,Southampton,n\r\n",
      "1,female,38.0,1,0,71.2833,First,C,Cherbourg,n\r\n",
      "1,female,26.0,0,0,7.925,Third,unknown,Southampton,y\r\n",
      "1,female,35.0,1,0,53.1,First,C,Southampton,n\r\n",
      "0,male,28.0,0,0,8.4583,Third,unknown,Queenstown,y\r\n",
      "0,male,2.0,3,1,21.075,Third,unknown,Southampton,n\r\n",
      "1,female,27.0,0,2,11.1333,Third,unknown,Southampton,n\r\n",
      "1,female,14.0,1,0,30.0708,Second,unknown,Cherbourg,n\r\n",
      "1,female,4.0,1,1,16.7,Third,G,Southampton,n\r\n"
     ]
    }
   ],
   "source": [
    "# 查看数据\n",
    "!head {train_file_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据\n",
    "\n",
    "从文件中读取 CSV 数据并且创建 dataset。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def get_dataset(file_path):\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        file_path,\n",
    "        batch_size=12, # 为了示例更容易展示，手动设置较小的值\n",
    "        label_name='survived',\n",
    "        na_value=\"?\",\n",
    "        num_epochs=1,\n",
    "        ignore_errors=True)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# 对训练数据进行shuffle\n",
    "train_data = get_dataset(train_file_path).shuffle(500)\n",
    "test_data = get_dataset(test_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理\n",
    "\n",
    "### 分类数据\n",
    "\n",
    "CSV 数据中的有些列是分类的列，也就是说，这些列只能在有限的集合中取值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex ['male' 'female']\n",
      "class ['Third' 'First' 'Second']\n",
      "deck ['unknown' 'C' 'G' 'A' 'B' 'D' 'F' 'E']\n",
      "embark_town ['Southampton' 'Cherbourg' 'Queenstown' 'unknown']\n",
      "alone ['n' 'y']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(train_file_path)\n",
    "# 获取各列的vocab\n",
    "for name in ['sex', 'class', 'deck', 'embark_town', 'alone']:\n",
    "    print(name, df[name].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各列的vocab\n",
    "CATEGORIES = {\n",
    "    'sex': ['male', 'female'],\n",
    "    'class' : ['First', 'Second', 'Third'],\n",
    "    'deck' : ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'],\n",
    "    'embark_town' : ['Cherbourg', 'Southhampton', 'Queenstown'],\n",
    "    'alone' : ['y', 'n']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = []\n",
    "for feature, vocab in CATEGORIES.items():\n",
    "    # 使用列名和vocab创建一个分类的列，即使用one-hot编码\n",
    "    cat_col = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "        key=feature, vocabulary_list=vocab)\n",
    "    categorical_columns.append(tf.feature_column.indicator_column(cat_col))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 连续数据\n",
    "\n",
    "连续数据需要标准化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_continuous(mean, data):\n",
    "    # 标准化连续数据\n",
    "    data = tf.cast(data, tf.float32) / mean\n",
    "    return tf.reshape(data, [-1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age 29.631307814992027\n",
      "n_siblings_spouses 0.5454545454545454\n",
      "parch 0.379585326953748\n",
      "fare 34.385398564593245\n"
     ]
    }
   ],
   "source": [
    "# 获取各列的均值\n",
    "for name in ['age', 'n_siblings_spouses', 'parch', 'fare']:\n",
    "    print(name, df[name].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各列的均值\n",
    "MEANS = {\n",
    "    'age' : 29.631308,\n",
    "    'n_siblings_spouses' : 0.545455,\n",
    "    'parch' : 0.379585,\n",
    "    'fare' : 34.385399\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "numerical_columns = []\n",
    "for feature in MEANS.keys():\n",
    "    # 使用normalizer_fn进行预处理\n",
    "    # functools.partial基于一个函数创建一个可调用对象，把原函数的某些参数固定。\n",
    "    num_col = tf.feature_column.numeric_column(feature, \n",
    "                                               normalizer_fn=\n",
    "                                               functools.partial(process_continuous, MEANS[feature]))\n",
    "    numerical_columns.append(num_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建预处理层\n",
    "\n",
    "合并分类数据和连续数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_layer = tf.keras.layers.DenseFeatures(categorical_columns+numerical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 24)\n"
     ]
    }
   ],
   "source": [
    "for X, y in test_data:\n",
    "    # shape is (batch_size, input_dim)\n",
    "    print(preprocessing_layer(X).shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加一个预处理层\n",
    "model = tf.keras.Sequential([\n",
    "  preprocessing_layer,\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid'),\n",
    "])\n",
    "# 编译\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.3957 - accuracy: 0.8293\n",
      "Epoch 2/5\n",
      "53/53 [==============================] - 0s 953us/step - loss: 0.3819 - accuracy: 0.8309\n",
      "Epoch 3/5\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.3806 - accuracy: 0.8421\n",
      "Epoch 4/5\n",
      "53/53 [==============================] - 0s 957us/step - loss: 0.3809 - accuracy: 0.8389\n",
      "Epoch 5/5\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.3684 - accuracy: 0.8517\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff7ad702b80>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 无视WARNING，官方文档也是有WARNING的\n",
    "model.fit(train_data, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
