{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  优化算法\n",
    "\n",
    "```{note}\n",
    "本节介绍tensorflow中各种常见的优化算法。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 激活函数和初始化\n",
    "\n",
    "选择合适的激活函数和初始化可以降低梯度消失和梯度爆炸的风险，从而让模型更好地训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "# 在层中定义激活函数和初始化\n",
    "layer = keras.layers.Dense(10, \n",
    "                           activation=\"relu\", \n",
    "                           kernel_initializer=\"he_normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# momentum\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "momentum 的速度，使用当前的梯度:\n",
    "\n",
    "$$\n",
    "\\mathbf{v} \\gets \\beta\\mathbf{v} + \\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta})\n",
    "$$\n",
    "\n",
    "Nesterov momentum 的速度，使用预估的下一步的梯度:\n",
    "\n",
    "$$\n",
    "\\mathbf{v} \\gets \\beta\\mathbf{v} + \\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta} - \\eta\\mathbf{v})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nesterov momentum\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSprop，梯度大时步幅小，梯度小时步幅大\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam: momentum + RMSprop\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nadam: nesterov momentum + RMSprop\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度截断\n",
    "\n",
    "若梯度的绝对值超过阈值则进行截断。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在定义优化器时指定截断\n",
    "optimizer = keras.optimizers.SGD(clipvalue=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学习率调度器\n",
    "\n",
    "学习率调度器是一类回调函数（callback）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100个batch没有improve就将学习率乘以0.5\n",
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=100)\n",
    "# 在优化器中定义\n",
    "optimizer = keras.optimizers.SGD(learning_rate=lr_scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{warning}\n",
    "注意这里是每个batch不是每个epoch。<br/>\n",
    "若需epoch-wise操作，可以像一般callback那样在训练时指定，后面会讲。\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
