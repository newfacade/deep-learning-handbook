{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 反向传播\n",
    "\n",
    "```{note}\n",
    "正向传播即计算输出<br/>\n",
    "反向传播即使用链式法则从输出到输入计算梯度\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正向传播\n",
    "\n",
    "搭建神经网络就像是搭乐高积木: \n",
    "\n",
    "![jupyter](../images/b/mlp.svg)\n",
    "\n",
    "我们用中括号标识层，比如说在上图中， $[0]$ 标识输入层, $[1]$ 标识隐藏层, $[2]$ 标识输出层\n",
    "\n",
    "$\\mathbf{a}^{[l]}$ 表示 $l$ 层的输出, 并令 $\\mathbf{a}^{[0]} = x$\n",
    "\n",
    "$\\mathbf{z}^{[l]}$ 表示 $l$ 层的仿射结果\n",
    "\n",
    "$g^{[l]}$ 表示 $l$ 层的激活函数\n",
    "\n",
    "正向传播即:\n",
    "\n",
    "$$\\mathbf{z}^{[l]} = \\mathbf{W}^{[l]}\\mathbf{a}^{[l-1]} + \\mathbf{b}^{[l]}$$\n",
    "\n",
    "$$\\mathbf{a}^{[l]} = g^{[l]}(\\mathbf{z}^{[l]})$$\n",
    "\n",
    "其中 $\\mathbf{W}^{[l]} \\in \\mathbb{R}^{d[l] \\times d[l-1]}$, $\\mathbf{b}^{[l]} \\in \\mathbb{R}^{d[l]}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预备知识\n",
    "\n",
    "1.假设在正向传播中 $\\mathbf{x} \\to \\mathbf{y} \\to L$, 其中 $L \\in \\mathbb{R}$是损失， $\\mathbf{x} \\in \\mathbb{R}^{n}$, $\\mathbf{y} \\in \\mathbb{R} ^{m}$ 比 $\\mathbf{x}$ 更靠近输出层:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{y}} = \\begin{bmatrix}\n",
    " \\frac{\\partial L}{\\partial y_{1}} \\\\\n",
    " ...\\\\\n",
    " \\frac{\\partial L}{\\partial y_{m}}\n",
    "\\end{bmatrix}\n",
    "\\quad\n",
    ",\n",
    "\\quad\n",
    "\\frac{\\partial L}{\\partial \\mathbf{x}} = \\begin{bmatrix}\n",
    " \\frac{\\partial L}{\\partial x_{1}} \\\\\n",
    " ...\\\\\n",
    " \\frac{\\partial L}{\\partial x_{n}}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "使用全微分公式:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x_{k}} = \\sum_{j=1}^{m}\\frac{\\partial L}{\\partial y_{j}}\\frac{\\partial y_{j}}{\\partial x_{k}}\n",
    "$$\n",
    "\n",
    "从而我们可以计算 $\\frac{\\partial L}{\\partial \\mathbf{x}}$ 和 $\\frac{\\partial L}{\\partial \\mathbf{y}}$ 的关系:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{x}} = \\begin{bmatrix}\n",
    " \\frac{\\partial L}{\\partial x_{1}} \\\\\n",
    " ...\\\\\n",
    " \\frac{\\partial L}{\\partial x_{n}}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    " \\frac{\\partial y_{1}}{\\partial x_{1}} & ... & \\frac{\\partial y_{m}}{\\partial x_{1}}\\\\\n",
    " \\vdots  & \\ddots  & \\vdots \\\\\n",
    "  \\frac{\\partial y_{1}}{\\partial x_{n}}& .... & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    " \\frac{\\partial L}{\\partial y_{1}} \\\\\n",
    " ...\\\\\n",
    " \\frac{\\partial L}{\\partial y_{m}}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "(\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}})^{T}\\frac{\\partial L}{\\partial \\mathbf{y}}\n",
    "$$\n",
    "\n",
    "这里 $\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}$ 是 jacobian 矩阵.\n",
    "\n",
    "2.矩阵乘法的 jacobian 矩阵，这很容易验证:\n",
    "\n",
    "$$\\frac{\\partial \\mathbf{M}\\mathbf{x}}{\\partial \\mathbf{x}}=\\mathbf{M}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 反向传播\n",
    "\n",
    "recall 梯度下降公式:\n",
    "\n",
    "$$\\mathbf{W}^{[l]} = \\mathbf{W}^{[l]} - \\alpha\\frac{\\partial{L}}{\\partial{\\mathbf{W}^{[l]}}}$$\n",
    "\n",
    "$$\\mathbf{b}^{[l]} = \\mathbf{b}^{[l]} - \\alpha\\frac{\\partial{L}}{\\partial{\\mathbf{b}^{[l]}}}$$\n",
    "\n",
    "我们需要计算 $L$ 对各参数的梯度\n",
    "\n",
    "分三步走:\n",
    "\n",
    "1.计算输出层的梯度 $\\frac{\\partial L}{\\partial \\mathbf{z}^{[N]}}$ :\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{z}^{[N]}} = (\\frac{\\partial \\mathbf{a}^{[N]}}{\\partial \\mathbf{z}^{[N]}})^{T}\\frac{\\partial L}{\\partial \\mathbf{a}^{[L]}}\n",
    "$$\n",
    "\n",
    "2.计算隐藏层的梯度 $\\frac{\\partial L}{\\partial \\mathbf{z}^{[l]}}, l=N-1,...,1$:\n",
    "\n",
    "$$\\mathbf{z}^{[l + 1]} = \\mathbf{W}^{[l + 1]}\\mathbf{a}^{[l]} + \\mathbf{b}^{[l + 1]}$$\n",
    "\n",
    "通过前面的预备知识我们知道:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{a}^{[l]}} = (\\frac{\\partial \\mathbf{z}^{[l+1]}}{\\partial \\mathbf{a}^{[l]}})^{T}\\frac{\\partial L}{\\partial \\mathbf{z}^{[l+1]}} = (\\mathbf{W}^{[l+1]})^{T}\\frac{\\partial L}{\\partial \\mathbf{z}^{[l+1]}}\n",
    "$$\n",
    "\n",
    "注意到隐藏层的激活函数 $g^{[l]}$ 不会相互依赖，因此:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{z}^{[l]}} = \\frac{\\partial L}{\\partial \\mathbf{a}^{[l]}} \\odot {g^{[l]}}'(\\mathbf{z}^{[l]})$$\n",
    "\n",
    "结合起来:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{z}^{[l]}} = (\\mathbf{W}^{[l+1]})^{T}\\frac{\\partial L}{\\partial \\mathbf{z}^{[l+1]}} \\odot {g^{[l]}}'(\\mathbf{z}^{[l]})$$\n",
    "\n",
    "3.计算参数的梯度 $\\frac{\\partial L}{\\partial \\mathbf{W}^{[l]}}$ 和 $\\frac{\\partial L}{\\partial \\mathbf{b}^{[l]}}$ for $l=N,...,1$:\n",
    "\n",
    "$$\\mathbf{z}^{[l]} = \\mathbf{W}^{[l]}\\mathbf{a}^{[l - 1]} + \\mathbf{b}^{[l]}$$\n",
    "\n",
    "通过链式法则可以得到:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{W}^{[l]}} = \\frac{\\partial L}{\\partial \\mathbf{z}^{[l]}}(\\mathbf{a}^{[l - 1]})^{T}$$ \n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{b}^{[l]}}=\\frac{\\partial L}{\\partial \\mathbf{z}^{[l]}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
