{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 正则化\n",
    "\n",
    "```{note}\n",
    "深度神经网络的拟合能力虽强，但代价是过拟合的风险<br/>\n",
    "从《机器学习手册》中我们就知道，正则化（regularization）是线性模型处理过拟合的有效手段，对于深度神经网络来说也是如此\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 什么是过拟合\n",
    "\n",
    "对于coder，过拟合就是模型在测试集的表现比训练集差很多\n",
    "\n",
    "这往往是由于模型专门拟合了此训练集的不属于整体分布的模式造成的\n",
    "\n",
    "![jupyter](../images/b/error.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuition\n",
    "\n",
    "一般地，对于同类模型其参数越多，参数的取值范围越大，其拟合能力就越强，也越容易过拟合。\n",
    "\n",
    "要平衡模型的拟合能力和过拟合的风险，往往可以在参数的取值范围上做文章，正则化就是通过对绝对值大的参数做惩罚，软性地限制参数的取值范围。\n",
    "\n",
    "在神经网络中，若模型的直接损失为 $L$，那么加上 $l_{1}$ 正则后的目标函数：\n",
    "\n",
    "$$L + \\lambda\\left \\|\\boldsymbol{\\theta}  \\right \\|_{1}$$\n",
    "\n",
    "加上 $l_{2}$ 正则后的目标函数：\n",
    "\n",
    "$$L + \\frac{\\lambda}{2}\\left \\|\\boldsymbol\\theta  \\right \\|_{2}^{2}$$\n",
    "\n",
    "若使用梯度下降，$l_{2}$ 正则化等价于权重衰减：\n",
    "\n",
    "$$\\boldsymbol\\theta_{t} = (1 - \\lambda\\eta)\\boldsymbol\\theta_{t - 1} - \\eta\\nabla\\boldsymbol{\\theta}_{t - 1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch中的正则化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "net = nn.Sequential(nn.Linear(784, 1))\n",
    "# pytorch可直接在optimizer中定义权重衰减，此时weight和bias都衰减\n",
    "# weight_decay默认为0\n",
    "lr = 0.1\n",
    "weight_decay = 0.01\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义不衰减bias\n",
    "optimizer = torch.optim.SGD([{\"params\":net[0].weight,'weight_decay': weight_decay},\n",
    "                             {\"params\":net[0].bias}], lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{warning}\n",
    "在Adam等较复杂的优化方法中，权重衰减不等价于l2正则化，不过是类似的，用就是了<br/>\n",
    "对于深度神经网络，正则化的效果不够显著，还需要其他方法（后两节会介绍）\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
