{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练机器翻译\n",
    "\n",
    "```{note}\n",
    "本节实现一个可以训练机器翻译模型的函数。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数\n",
    "\n",
    "每个时间步，解码器预测了输出词元的概率分布。类似于语言模型，可以使用softmax获得分布，并通过计算交叉熵损失来进行优化。\n",
    "\n",
    "但是填充词元应该被排除在损失函数的计算之外。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import d2l\n",
    "\n",
    "\n",
    "#@save\n",
    "def sequence_mask(X, valid_len, value=0):\n",
    "    \"\"\"在序列中屏蔽不想关的项\"\"\"\n",
    "    # `X` shape: (`batch_size`, `num_steps`)\n",
    "    # [None, :] makes (`num_steps`,) to (1, `num_steps`)\n",
    "    # [:, None] makes (`batch_size`) to (`batch_size`, 1)\n",
    "    mask = torch.arange((X.size(1)), dtype=torch.float32,\n",
    "                        device=X.device)[None, :] < valid_len[:, None]\n",
    "    X[~mask] = value\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):\n",
    "    \"\"\"带屏蔽的softmax交叉熵损失函数\"\"\"\n",
    "    # shape of pred: (`batch_size`, `num_steps`, `vocab_size`)\n",
    "    # shape of label: (`batch_size`, `num_steps`)\n",
    "    # shape of valid_len: (`batch_size`,)\n",
    "    def forward(self, pred, label, valid_len):\n",
    "        # 非pad为1，pad为0\n",
    "        weights = torch.ones_like(label)\n",
    "        weights = sequence_mask(weights, valid_len)\n",
    "        # 'none': no reduction will be applied, 'mean': the weighted mean of the output is taken\n",
    "        self.reduction = 'none'\n",
    "        # nn.CrossEntropyLoss((`batch_size`, `vocab_size`, `num_steps`), (`batch_size`, `num_steps`))\n",
    "        unweighted_loss = super().forward(pred.permute(0, 2, 1), label)\n",
    "        # 得到带屏蔽的损失\n",
    "        weighted_loss = (unweighted_loss * weights).mean(dim=1)\n",
    "        return weighted_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练\n",
    "\n",
    "训练时，我们解码器的输入不是采样自上一步的输出，而是`<bos>` + 真实的输出序列，这被称为teacher-forcing，它能使我们训练得更快。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def train_nmt(net, data_iter, lr, num_epochs, tgt_vocab):\n",
    "    \"\"\"训练机器翻译模型\"\"\"\n",
    "    device = d2l.try_gpu()\n",
    "    net.to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    loss_fn = MaskedSoftmaxCELoss()\n",
    "    net.train()  # 用了Dropout，必须明示\n",
    "    # 画带屏蔽的交叉熵损失\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n",
    "                            xlim=[10, num_epochs])\n",
    "    for epoch in range(num_epochs):\n",
    "        # 损失和，tokens总数\n",
    "        metric = d2l.Accumulator(2)\n",
    "        for batch in data_iter:\n",
    "            X, X_valid_len, Y, Y_valid_len = [x.to(device) for x in batch]\n",
    "            # 解码器的输入是<bos>+真实输出序列 \n",
    "            bos = torch.tensor([tgt_vocab['<bos>']] * Y.shape[0],\n",
    "                               device=device).reshape(-1, 1)\n",
    "            dec_input = torch.cat([bos, Y[:, :-1]], 1)\n",
    "            # 模型需是Encoder-Decoder结构\n",
    "            Y_hat, _ = net(X, dec_input, X_valid_len)\n",
    "            \n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(Y_hat, Y, Y_valid_len)\n",
    "            loss.sum().backward()\n",
    "            optimizer.step()\n",
    "            # 记录数据\n",
    "            with torch.no_grad():\n",
    "                metric.add(loss.sum(), Y_valid_len.sum())\n",
    "        # 画图\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            animator.add(epoch + 1, (metric[0] / metric[1],))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测\n",
    "\n",
    "预测时，我们没有真实的输出序列，解码器当前时间步的输入都将来自于前一时间步的输出词元。\n",
    "\n",
    "![jupyter](../images/9/seq2seq-predict.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def predict_nmt(net, src_sentence, src_vocab, tgt_vocab, num_steps, device=d2l.try_gpu()):\n",
    "    \"\"\"机器翻译模型做预测\"\"\"\n",
    "    net.eval()\n",
    "    # 处理src_sentence\n",
    "    src_tokens = src_vocab[src_sentence.lower().split(' ')] + [src_vocab['<eos>']]\n",
    "    enc_valid_len = torch.tensor([len(src_tokens)], device=device)\n",
    "    src_tokens = d2l.truncate_pad(src_tokens, num_steps, src_vocab['<pad>'])\n",
    "    enc_X = torch.unsqueeze(\n",
    "        torch.tensor(src_tokens, dtype=torch.long, device=device), dim=0)\n",
    "    enc_outputs = net.encoder(enc_X, enc_valid_len)\n",
    "    # 解码器初始state及初始输入\n",
    "    dec_state = net.decoder.init_state(enc_outputs, enc_valid_len)\n",
    "    dec_X = torch.unsqueeze(\n",
    "        torch.tensor([tgt_vocab['<bos>']], dtype=torch.long, device=device), dim=0)\n",
    "    output_seq, attention_weight_seq = [], []\n",
    "    # 一步一步来\n",
    "    for _ in range(num_steps):\n",
    "        Y, dec_state = net.decoder(dec_X, dec_state)\n",
    "        # We use the token with the highest prediction likelihood as the input\n",
    "        # of the decoder at the next time step\n",
    "        dec_X = Y.argmax(dim=2)\n",
    "        pred = dec_X.squeeze(dim=0).type(torch.int32).item()\n",
    "        # Once the end-of-sequence token is predicted, the generation of the\n",
    "        # output sequence is complete\n",
    "        if pred == tgt_vocab['<eos>']:\n",
    "            break\n",
    "        output_seq.append(pred)\n",
    "    return ' '.join(tgt_vocab.to_tokens(output_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估\n",
    "\n",
    "我们可以通过与真实标签序列做比较来评估预测序列。\n",
    "\n",
    "用 $p_{n}$ 表示 $n$元语法的精确度，它是两个数量的比值，分子是预测序列与标签序列中匹配的 $n$元语法的数量，分母是预测序列中 $n$元语法的数量。\n",
    "\n",
    "那么, BLEU 的定义是:\n",
    "\n",
    "$$\\exp\\left(\\min\\left(0, 1 - \\frac{\\mathrm{len}_{\\text{label}}}{\\mathrm{len}_{\\text{pred}}}\\right)\\right)\\prod_{i=1}^{k}p_{n}^{1/{2^{n}}}$$\n",
    "\n",
    "其中 $k$ 是用于匹配的最长 $n$元语法，指数项用于惩罚较短的预测序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def bleu(pred_seq, label_seq, k):\n",
    "    \"\"\"计算 BLEU\"\"\"\n",
    "    pred_tokens, label_tokens = pred_seq.split(' '), label_seq.split(' ')\n",
    "    len_pred, len_label = len(pred_tokens), len(label_tokens)\n",
    "    score = math.exp(min(0, 1 - len_label / len_pred))\n",
    "    # 计算n元语法的精确度\n",
    "    for n in range(1, k + 1):\n",
    "        num_matches, label_subs = 0, collections.defaultdict(int)\n",
    "        # 统计标签序列中各n元语法的数量\n",
    "        for i in range(len_label - n + 1):\n",
    "            label_subs[''.join(label_tokens[i:i + n])] += 1\n",
    "        # 计算匹配\n",
    "        for i in range(len_pred - n + 1):\n",
    "            if label_subs[''.join(pred_tokens[i:i + n])] > 0:\n",
    "                num_matches += 1\n",
    "                label_subs[''.join(pred_tokens[i:i + n])] -= 1\n",
    "        score *= math.pow(num_matches / (len_pred - n + 1), math.pow(0.5, n))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
